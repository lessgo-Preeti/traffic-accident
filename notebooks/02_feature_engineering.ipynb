{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7e2b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset: (3000, 22)\n",
      "\n",
      "Columns:\n",
      "['State Name', 'City Name', 'Year', 'Month', 'Day of Week', 'Time of Day', 'Accident Severity', 'Number of Vehicles Involved', 'Vehicle Type Involved', 'Number of Casualties', 'Number of Fatalities', 'Weather Conditions', 'Road Type', 'Road Condition', 'Lighting Conditions', 'Traffic Control Presence', 'Speed Limit (km/h)', 'Driver Age', 'Driver Gender', 'Driver License Status', 'Alcohol Involvement', 'Accident Location Details']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\LOQ\\\\traffic-accident\\\\data\\\\accident_prediction_india.csv')\n",
    "\n",
    "print(f\"Original Dataset: {df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51427beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Important Columns:\n",
      "============================================================\n",
      "target         : Accident Severity\n",
      "year           : Year\n",
      "month          : Month\n",
      "time           : Day of Week\n",
      "datetime       : Time of Day\n",
      "weather        : Weather Conditions\n",
      "location       : Road Type\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect important columns\n",
    "columns_map = {}\n",
    "\n",
    "# Find target column\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'severity' in col_lower:\n",
    "        columns_map['target'] = col\n",
    "        break\n",
    "\n",
    "# Find time columns\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'date' in col_lower or 'time' in col_lower:\n",
    "        columns_map['datetime'] = col\n",
    "        break\n",
    "    if 'year' in col_lower:\n",
    "        columns_map['year'] = col\n",
    "    if 'month' in col_lower:\n",
    "        columns_map['month'] = col\n",
    "    if 'day' in col_lower or 'hour' in col_lower:\n",
    "        columns_map['time'] = col\n",
    "\n",
    "# Find weather column\n",
    "for col in df.columns:\n",
    "    if 'weather' in col.lower():\n",
    "        columns_map['weather'] = col\n",
    "        break\n",
    "\n",
    "# Find location/road columns\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if 'road' in col_lower or 'location' in col_lower or 'area' in col_lower:\n",
    "        columns_map['location'] = col\n",
    "        break\n",
    "\n",
    "print(\"Detected Important Columns:\")\n",
    "print(\"=\"*60)\n",
    "for key, val in columns_map.items():\n",
    "    print(f\"{key:15s}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c281e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Missing Values...\n",
      "============================================================\n",
      "✓ Traffic Control Presence: Filled with mode\n",
      "✓ Driver License Status: Filled with mode\n",
      "\n",
      "✓ Missing values handled. New shape: (3000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create a working copy\n",
    "df_work = df.copy()\n",
    "\n",
    "print(\"Handling Missing Values...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fill missing values based on column type\n",
    "for col in df_work.columns:\n",
    "    if df_work[col].isnull().sum() > 0:\n",
    "        if df_work[col].dtype in ['int64', 'float64']:\n",
    "            # Numeric: fill with median\n",
    "            df_work[col].fillna(df_work[col].median(), inplace=True)\n",
    "            print(f\"✓ {col}: Filled with median\")\n",
    "        else:\n",
    "            # Categorical: fill with mode or 'Unknown'\n",
    "            mode_val = df_work[col].mode()\n",
    "            if len(mode_val) > 0:\n",
    "                df_work[col].fillna(mode_val[0], inplace=True)\n",
    "                print(f\"✓ {col}: Filled with mode\")\n",
    "            else:\n",
    "                df_work[col].fillna('Unknown', inplace=True)\n",
    "                print(f\"✓ {col}: Filled with 'Unknown'\")\n",
    "\n",
    "print(f\"\\n✓ Missing values handled. New shape: {df_work.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69494a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Time Features...\n",
      "============================================================\n",
      "✓ Extracted: hour, day_of_week, month, year\n",
      "✓ Created: is_weekend\n",
      "✓ Created: is_rush_hour, is_night\n",
      "✓ Created: is_monsoon\n",
      "\n",
      "✓ Time features created. Shape: (3000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Time Features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# If datetime column exists\n",
    "if 'datetime' in columns_map:\n",
    "    try:\n",
    "        df_work['datetime_parsed'] = pd.to_datetime(df_work[columns_map['datetime']], errors='coerce')\n",
    "        \n",
    "        df_work['hour'] = df_work['datetime_parsed'].dt.hour\n",
    "        df_work['day_of_week'] = df_work['datetime_parsed'].dt.dayofweek\n",
    "        df_work['month'] = df_work['datetime_parsed'].dt.month\n",
    "        df_work['year'] = df_work['datetime_parsed'].dt.year\n",
    "        \n",
    "        print(\"✓ Extracted: hour, day_of_week, month, year\")\n",
    "    except:\n",
    "        print(\"✗ Could not parse datetime column\")\n",
    "\n",
    "# If separate columns exist\n",
    "else:\n",
    "    if 'year' in columns_map:\n",
    "        df_work['year'] = df_work[columns_map['year']]\n",
    "        print(f\"✓ Using existing year column\")\n",
    "    \n",
    "    if 'month' in columns_map:\n",
    "        df_work['month'] = df_work[columns_map['month']]\n",
    "        print(f\"✓ Using existing month column\")\n",
    "\n",
    "# Create derived features\n",
    "if 'day_of_week' in df_work.columns:\n",
    "    df_work['is_weekend'] = (df_work['day_of_week'] >= 5).astype(int)\n",
    "    print(\"✓ Created: is_weekend\")\n",
    "\n",
    "if 'hour' in df_work.columns:\n",
    "    df_work['is_rush_hour'] = df_work['hour'].isin([7, 8, 9, 17, 18, 19]).astype(int)\n",
    "    df_work['is_night'] = df_work['hour'].isin(range(22, 24)) | df_work['hour'].isin(range(0, 6))\n",
    "    df_work['is_night'] = df_work['is_night'].astype(int)\n",
    "    print(\"✓ Created: is_rush_hour, is_night\")\n",
    "\n",
    "if 'month' in df_work.columns:\n",
    "    # Monsoon season in India (June-September)\n",
    "    df_work['is_monsoon'] = df_work['month'].isin([6, 7, 8, 9]).astype(int)\n",
    "    print(\"✓ Created: is_monsoon\")\n",
    "\n",
    "print(f\"\\n✓ Time features created. Shape: {df_work.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1056bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Weather Features...\n",
      "============================================================\n",
      "Weather Categories:\n",
      "weather_main\n",
      "Rain     631\n",
      "Storm    611\n",
      "Other    608\n",
      "Fog      576\n",
      "Clear    574\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Weather mapped to: Clear, Rain, Fog, Cloudy, Snow, Storm, Other\n",
      "\n",
      "✓ Weather features processed. Shape: (3000, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Weather Features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'weather' in columns_map:\n",
    "    weather_col = columns_map['weather']\n",
    "    \n",
    "    # Map to broad categories\n",
    "    def map_weather(condition):\n",
    "        if pd.isna(condition):\n",
    "            return 'Clear'\n",
    "        \n",
    "        c = str(condition).lower()\n",
    "        \n",
    "        if any(x in c for x in ['clear', 'fair', 'sunny', 'fine']):\n",
    "            return 'Clear'\n",
    "        elif any(x in c for x in ['rain', 'drizzle', 'shower', 'wet']):\n",
    "            return 'Rain'\n",
    "        elif any(x in c for x in ['fog', 'mist', 'haze', 'smoke']):\n",
    "            return 'Fog'\n",
    "        elif any(x in c for x in ['cloud', 'overcast']):\n",
    "            return 'Cloudy'\n",
    "        elif any(x in c for x in ['snow', 'ice', 'sleet', 'hail']):\n",
    "            return 'Snow'\n",
    "        elif any(x in c for x in ['storm', 'thunder', 'wind']):\n",
    "            return 'Storm'\n",
    "        else:\n",
    "            return 'Other'\n",
    "    \n",
    "    df_work['weather_main'] = df_work[weather_col].apply(map_weather)\n",
    "    \n",
    "    print(\"Weather Categories:\")\n",
    "    print(df_work['weather_main'].value_counts())\n",
    "    print(\"\\n✓ Weather mapped to: Clear, Rain, Fog, Cloudy, Snow, Storm, Other\")\n",
    "else:\n",
    "    # Create default weather column\n",
    "    df_work['weather_main'] = 'Clear'\n",
    "    print(\"⚠ No weather column found. Created default 'Clear' weather\")\n",
    "\n",
    "print(f\"\\n✓ Weather features processed. Shape: {df_work.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60b55a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Road/Location Features...\n",
      "============================================================\n",
      "Road Type Distribution:\n",
      "road_type\n",
      "highway    1520\n",
      "rural       768\n",
      "urban       712\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Road types: highway, arterial, urban, rural, residential, unknown\n",
      "⚠ Created default: is_junction = 0\n",
      "✓ Created: is_urban\n",
      "⚠ Created default: is_highway = 0\n",
      "\n",
      "✓ Road features processed. Shape: (3000, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Road/Location Features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'location' in columns_map:\n",
    "    location_col = columns_map['location']\n",
    "    \n",
    "    # Map to road types\n",
    "    def classify_road(location):\n",
    "        if pd.isna(location):\n",
    "            return 'unknown'\n",
    "        \n",
    "        loc = str(location).lower()\n",
    "        \n",
    "        if any(x in loc for x in ['highway', 'expressway', 'nh-', 'national']):\n",
    "            return 'highway'\n",
    "        elif any(x in loc for x in ['main', 'arterial', 'state', 'sh-']):\n",
    "            return 'arterial'\n",
    "        elif any(x in loc for x in ['urban', 'city', 'town', 'metro']):\n",
    "            return 'urban'\n",
    "        elif any(x in loc for x in ['rural', 'village', 'gram']):\n",
    "            return 'rural'\n",
    "        elif any(x in loc for x in ['residential', 'colony', 'sector']):\n",
    "            return 'residential'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    df_work['road_type'] = df_work[location_col].apply(classify_road)\n",
    "    \n",
    "    print(\"Road Type Distribution:\")\n",
    "    print(df_work['road_type'].value_counts())\n",
    "    print(\"\\n✓ Road types: highway, arterial, urban, rural, residential, unknown\")\n",
    "else:\n",
    "    df_work['road_type'] = 'unknown'\n",
    "    print(\"⚠ No location column found. Created default 'unknown' road_type\")\n",
    "\n",
    "# Additional binary features (if columns exist)\n",
    "for feature_name, keywords in [\n",
    "    ('is_junction', ['junction', 'intersection', 'crossing', 'chowk']),\n",
    "    ('is_urban', ['urban', 'city', 'town', 'metro']),\n",
    "    ('is_highway', ['highway', 'expressway', 'nh-', 'national'])\n",
    "]:\n",
    "    found = False\n",
    "    for col in df_work.columns:\n",
    "        if any(kw in col.lower() for kw in keywords):\n",
    "            if df_work[col].dtype == 'object':\n",
    "                df_work[feature_name] = df_work[col].notna().astype(int)\n",
    "            else:\n",
    "                df_work[feature_name] = (df_work[col] > 0).astype(int)\n",
    "            found = True\n",
    "            print(f\"✓ Created: {feature_name}\")\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        df_work[feature_name] = 0\n",
    "        print(f\"⚠ Created default: {feature_name} = 0\")\n",
    "\n",
    "print(f\"\\n✓ Road features processed. Shape: {df_work.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781106f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Target Variable...\n",
      "============================================================\n",
      "Original Accident Severity distribution:\n",
      "Accident Severity\n",
      "Minor      1034\n",
      "Fatal       985\n",
      "Serious     981\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary Target Distribution:\n",
      "target_binary\n",
      "1    2015\n",
      "0     985\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class Balance: {1: 0.6716666666666666, 0: 0.3283333333333333}\n",
      "\n",
      "✓ Target variable created. Shape: (3000, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Target Variable...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'target' in columns_map:\n",
    "    target_col = columns_map['target']\n",
    "    \n",
    "    print(f\"Original {target_col} distribution:\")\n",
    "    print(df_work[target_col].value_counts())\n",
    "    \n",
    "    # Convert to binary: High Risk (1) vs Low Risk (0)\n",
    "    # Adjust threshold based on your data\n",
    "    unique_vals = sorted(df_work[target_col].unique())\n",
    "    \n",
    "    if len(unique_vals) <= 3:\n",
    "        # If 1,2,3 or similar: 3 = high, 1-2 = low\n",
    "        threshold = unique_vals[int(len(unique_vals) * 0.6)]  # Top 40% as high risk\n",
    "        df_work['target_binary'] = (df_work[target_col] >= threshold).astype(int)\n",
    "    else:\n",
    "        # For string categories, map manually\n",
    "        severity_map = {\n",
    "            'fatal': 1, 'serious': 1, 'severe': 1, 'major': 1, 'high': 1,\n",
    "            'slight': 0, 'minor': 0, 'low': 0, 'moderate': 0\n",
    "        }\n",
    "        \n",
    "        def map_severity(val):\n",
    "            val_lower = str(val).lower()\n",
    "            for key, result in severity_map.items():\n",
    "                if key in val_lower:\n",
    "                    return result\n",
    "            return 0  # Default to low risk\n",
    "        \n",
    "        df_work['target_binary'] = df_work[target_col].apply(map_severity)\n",
    "    \n",
    "    print(f\"\\nBinary Target Distribution:\")\n",
    "    print(df_work['target_binary'].value_counts())\n",
    "    print(f\"\\nClass Balance: {df_work['target_binary'].value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No target column found! Using first categorical as target\")\n",
    "    cat_cols = df_work.select_dtypes(include=['object']).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        df_work['target_binary'] = (df_work[cat_cols[0]] == df_work[cat_cols[0]].value_counts().index[0]).astype(int)\n",
    "    else:\n",
    "        df_work['target_binary'] = 0\n",
    "\n",
    "print(f\"\\n✓ Target variable created. Shape: {df_work.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58758e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting Final Features...\n",
      "============================================================\n",
      "Selected Features (19):\n",
      "============================================================\n",
      " 1. hour                 | Type: float64    | Unique: 24\n",
      " 2. day_of_week          | Type: float64    | Unique: 1\n",
      " 3. month                | Type: float64    | Unique: 1\n",
      " 4. is_weekend           | Type: int32      | Unique: 1\n",
      " 5. is_rush_hour         | Type: int32      | Unique: 2\n",
      " 6. is_night             | Type: int32      | Unique: 2\n",
      " 7. is_monsoon           | Type: int32      | Unique: 1\n",
      " 8. weather_main         | Type: object     | Unique: 5\n",
      " 9. road_type            | Type: object     | Unique: 3\n",
      "10. is_junction          | Type: int64      | Unique: 1\n",
      "11. is_urban             | Type: int32      | Unique: 1\n",
      "12. is_highway           | Type: int64      | Unique: 1\n",
      "13. Year                 | Type: int64      | Unique: 6\n",
      "14. Number of Vehicles Involved | Type: int64      | Unique: 5\n",
      "15. Number of Casualties | Type: int64      | Unique: 11\n",
      "16. Number of Fatalities | Type: int64      | Unique: 6\n",
      "17. Speed Limit (km/h)   | Type: int64      | Unique: 91\n",
      "18. Driver Age           | Type: int64      | Unique: 53\n",
      "19. year                 | Type: float64    | Unique: 1\n",
      "\n",
      "✓ Final Dataset Shape: (2506, 20)\n",
      "✓ Target Balance: {1: 1673, 0: 833}\n"
     ]
    }
   ],
   "source": [
    "print(\"Selecting Final Features...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = []\n",
    "\n",
    "# Time features\n",
    "time_features = ['hour', 'day_of_week', 'month', 'is_weekend', 'is_rush_hour', 'is_night', 'is_monsoon']\n",
    "for feat in time_features:\n",
    "    if feat in df_work.columns:\n",
    "        feature_columns.append(feat)\n",
    "\n",
    "# Weather features\n",
    "if 'weather_main' in df_work.columns:\n",
    "    feature_columns.append('weather_main')\n",
    "\n",
    "# Road/location features\n",
    "road_features = ['road_type', 'is_junction', 'is_urban', 'is_highway']\n",
    "for feat in road_features:\n",
    "    if feat in df_work.columns:\n",
    "        feature_columns.append(feat)\n",
    "\n",
    "# Add any remaining numeric columns (but exclude target and datetime)\n",
    "exclude_cols = ['target_binary'] + [columns_map.get('target', ''), columns_map.get('datetime', ''), \n",
    "                'datetime_parsed', columns_map.get('weather', ''), columns_map.get('location', '')]\n",
    "\n",
    "numeric_cols = df_work.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if col not in feature_columns and col not in exclude_cols and 'unnamed' not in col.lower():\n",
    "        feature_columns.append(col)\n",
    "        if len(feature_columns) > 20:  # Limit total features\n",
    "            break\n",
    "\n",
    "# Clean and prepare final dataset\n",
    "df_final = df_work[feature_columns + ['target_binary']].copy()\n",
    "\n",
    "# Drop any remaining NaN\n",
    "df_final = df_final.dropna()\n",
    "\n",
    "print(f\"Selected Features ({len(feature_columns)}):\")\n",
    "print(\"=\"*60)\n",
    "for i, feat in enumerate(feature_columns, 1):\n",
    "    dtype = df_final[feat].dtype\n",
    "    unique = df_final[feat].nunique()\n",
    "    print(f\"{i:2d}. {feat:20s} | Type: {str(dtype):10s} | Unique: {unique}\")\n",
    "\n",
    "print(f\"\\n✓ Final Dataset Shape: {df_final.shape}\")\n",
    "print(f\"✓ Target Balance: {df_final['target_binary'].value_counts().to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "935caff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Dataset...\n",
      "============================================================\n",
      "✓ Train Set: (2004, 20) | Target: {1: 1338, 0: 666}\n",
      "✓ Test Set:  (502, 20) | Target: {1: 335, 0: 167}\n",
      "\n",
      "============================================================\n",
      "✓ Files Saved:\n",
      "  - ../data/train.csv\n",
      "  - ../data/test.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Splitting Dataset...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_final.drop('target_binary', axis=1)\n",
    "y = df_final['target_binary']\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Create train and test dataframes\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv('../data/train.csv', index=False)\n",
    "test_df.to_csv('../data/test.csv', index=False)\n",
    "\n",
    "print(f\"✓ Train Set: {train_df.shape} | Target: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"✓ Test Set:  {test_df.shape} | Target: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Files Saved:\")\n",
    "print(\"  - ../data/train.csv\")\n",
    "print(\"  - ../data/test.csv\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff3dc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Metadata:\n",
      "============================================================\n",
      "{\n",
      "  \"total_features\": 19,\n",
      "  \"feature_list\": [\n",
      "    \"hour\",\n",
      "    \"day_of_week\",\n",
      "    \"month\",\n",
      "    \"is_weekend\",\n",
      "    \"is_rush_hour\",\n",
      "    \"is_night\",\n",
      "    \"is_monsoon\",\n",
      "    \"weather_main\",\n",
      "    \"road_type\",\n",
      "    \"is_junction\",\n",
      "    \"is_urban\",\n",
      "    \"is_highway\",\n",
      "    \"Year\",\n",
      "    \"Number of Vehicles Involved\",\n",
      "    \"Number of Casualties\",\n",
      "    \"Number of Fatalities\",\n",
      "    \"Speed Limit (km/h)\",\n",
      "    \"Driver Age\",\n",
      "    \"year\"\n",
      "  ],\n",
      "  \"categorical_features\": [\n",
      "    \"weather_main\",\n",
      "    \"road_type\"\n",
      "  ],\n",
      "  \"numerical_features\": [\n",
      "    \"hour\",\n",
      "    \"day_of_week\",\n",
      "    \"month\",\n",
      "    \"is_junction\",\n",
      "    \"is_highway\",\n",
      "    \"Year\",\n",
      "    \"Number of Vehicles Involved\",\n",
      "    \"Number of Casualties\",\n",
      "    \"Number of Fatalities\",\n",
      "    \"Speed Limit (km/h)\",\n",
      "    \"Driver Age\",\n",
      "    \"year\"\n",
      "  ],\n",
      "  \"target_column\": \"target_binary\",\n",
      "  \"train_size\": 2004,\n",
      "  \"test_size\": 502,\n",
      "  \"class_distribution\": {\n",
      "    \"1\": 1673,\n",
      "    \"0\": 833\n",
      "  }\n",
      "}\n",
      "\n",
      "✓ Metadata saved: models/feature_metadata.json\n",
      "\n",
      "======================================================================\n",
      "                ✓✓✓ FEATURE ENGINEERING COMPLETE! ✓✓✓                 \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create feature metadata\n",
    "feature_metadata = {\n",
    "    'total_features': len(feature_columns),\n",
    "    'feature_list': feature_columns,\n",
    "    'categorical_features': [col for col in feature_columns if df_final[col].dtype == 'object'],\n",
    "    'numerical_features': [col for col in feature_columns if df_final[col].dtype in ['int64', 'float64']],\n",
    "    'target_column': 'target_binary',\n",
    "    'train_size': train_df.shape[0],\n",
    "    'test_size': test_df.shape[0],\n",
    "    'class_distribution': y.value_counts().to_dict()\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open('../models/feature_metadata.json', 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "\n",
    "print(\"Feature Metadata:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(feature_metadata, indent=2))\n",
    "\n",
    "print(\"\\n✓ Metadata saved: models/feature_metadata.json\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓✓✓ FEATURE ENGINEERING COMPLETE! ✓✓✓\".center(70))\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7c924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
